===
局限

为了更好地理解 数据压缩算法，我们需要了解它们的一些局限性。
研究人员已经为此 打下了 完整而重要的理论基础，本节的最后 会简要讨论，
但是现在 我们先来探讨一个 方便入门的结论。

==
通用数据压缩

在已经学习了许多重要问题的算法之后，你可能会认为
我们的目标是 通用性的数据压缩算法，也就是说，一个能够缩小“任意比特流”的算法。

但与之相反，我们定下的目标更加朴素，因为通用性的数据压缩 是不可能存在的。

命题S 不存在能够压缩任意比特流的算法；
证明👇

我们来看两种 有见地的证明。
第一种采用的是 反证法：
假设存在一个 能够压缩“任意比特流”的算法，那么 也就能够用它压缩“自己的输出”
进而 得到一段“更短的比特流”，如此循环往复 直到最终得到的比特流的长度为0！
能够“将任意比特流的长度压缩为0”显然是荒谬的，因此 存在“能够压缩任意比特流“的算法的假设 也是错误的。

第二种证明方法基于统计：
假设有一种算法 能够对所有长度为1000位的比特流 进行无损压缩，
那么 每一种能够被压缩的比特流 都对应着一段 较短且不同的比特流。
但长度小于1000位的比特流 一共只有 1+2+4+...+2^999=2^1000-1种，
而长度位1000位的比特流一共有 2^1000种。
因此 该算法不可能压缩所有长度为1000的比特流??
如果我们声明更多的条件，那么这段证明 会更有说服力。
例如，继续假设 算法的目标是 取得大于50%的压缩率，
那么 显然所有长度为1000位的比特流中的压缩成功率 将只有1/2^500!

换句话说，对于任意数据压缩算法，将长度位1000位的随机比特流 压缩为一半的概率最多为 1/2^500.
当 遇到一种新的无损压缩算法时，我们可以肯定它是 无法大幅度压缩 随机比特流的。

抛弃 对压缩”随机比特流“的幻想 是理解数据压缩的起点。
虽然我们会经常处理 数百万 至 数十亿比特长度的字符串，
但是 处理过的数据总量 只是这种字符串总数的九牛一毛，
所以不必为了这个理论结果而沮丧。
事实上，经常被处理的比特字符串 都是非常有规律的 - 在压缩时，可以利用这一点。