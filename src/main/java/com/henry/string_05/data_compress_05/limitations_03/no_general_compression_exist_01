===
局限

为了更好地理解 数据压缩算法，我们 需要了解 它们的一些局限性。
研究人员 已经为此 打下了 完整而重要的理论基础，本节的最后 会简要讨论，
但是现在 我们 先来探讨一个 方便入门的结论。

==
通用数据压缩

在 已经学习了 许多重要问题的算法 之后，你可能会认为
我们的目标是 通用性的数据压缩算法，也就是说，一个能够缩小“任意比特流”的算法。

但与之相反，我们 定下的目标 更加朴素，因为 通用性的数据压缩 是 不可能存在的。

命题S 不存在 能够压缩 任意比特流的算法；
证明👇

我们来看两种 有见地的证明。
第一种 采用的是 反证法：
假设 存在一个 能够压缩“任意比特流”的算法，那么 也就能够 用它压缩“自己的输出”
进而 得到一段“更短的比特流”，如此 循环往复 直到 最终得到的 比特流的长度为0！
能够“将 任意比特流的长度 压缩为0”显然是 荒谬的，因此 存在“能够压缩 任意比特流“的算法 的假设 也是错误的。

第二种证明方法 基于统计：
假设有一种算法 能够 对 所有长度为1000位的比特流 进行 无损压缩，
那么 每一种 能够被压缩的比特流 都对应着一段 较短且不同的比特流。
但 长度小于1000位的比特流 一共只有 1+2+4+...+2^999=2^1000-1种，
而 长度为1000位的比特流 一共有 2^1000种。
因此 该算法 不可能压缩 所有长度为1000的比特流??
如果我们 声明 更多的条件，那么 这段证明 会更有说服力。
例如，继续假设 算法的目标是 取得大于50%的压缩率，
那么 显然 所有长度为1000位的比特流中的压缩成功率 将只有1/2^500!

换句话说，对于 任意 数据压缩算法，将 长度位1000位的随机比特流 压缩为一半的概率 最多为 1/2^500.
当 遇到一种 新的无损压缩算法 时，我们 可以肯定 它是 无法大幅度压缩 随机比特流的。

抛弃 对压缩”随机比特流“的幻想 是 理解数据压缩的起点。
虽然我们 会经常处理 数百万 至 数十亿比特长度的字符串，
但是 处理过的数据总量 只是 这种字符串总数的九牛一毛，所以 不必 为了 这个理论结果 而沮丧。
事实上，经常被处理的 比特字符串 都是 非常有规律的 - 在 压缩 时，可以利用这一点。