在上述命题中，我们使用的基本假设是：随机键模型；
但是对于这样一种假设，我们需要考察其合理性

也就是说 随机键模型 与 典型的/常用的符号表使用情况 是不是相符合的呢？
结论：对于大多数用例，随机键模型都能很好地适应它们的实验结果。

用于验证/支持结论的具体实验：
研究使用 CountWordFrequencyViaSymbolTable 处理 “命令行参数文件”中存在的“长度大于等于8的单词”时，put()操作的成本。

目标：比较不同符号表的实现的性能；
手段：使用一个符号表的具体用例，然后从符号表的实现01 切换到 实现02，从而对比实现01 与 实现02在某一操作上具体的性能表现。
过程：略（稍后补上）
结论：
    #1 当使用 BinarySearchTreeSymbolTable符号表实现 时，put()操作的平均成本 是 13次数组访问；
    #2 当使用 OrderedArraySymbolTable符号表实现 时，put()操作的平均成本 是 484次数组访问；
推论：BinarySearchTreeSymbolTable符号表实现 为put()算法提供了更好的性能；
作用：实验结果 验证了 理论模型所预测的 对数级别的性能。

-- 分析理论预测 与 实验数据之间存在差异的原因 --
按照理论预测，put()操作的平均成本 = 符号表大小的自然对数 * 2；
原理：对于一个几乎充满的符号表，针对它的大多数操作都是 查找操作。

导致预测不准确的可能原因👇
#1 很多操作其实是在 比较小的符号表中进行的；
#2 真实输入模型中，键可能并不是随机的；
#3 如果符号表本身太小的话，近似值 2lnN可能是不准确的。

疑问：怎么通过实验 来 获取到put()操作实际所需要的比较次数呢？？？



